{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep MHFL for input into the processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hengin\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import unidecode\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from ordered_set import OrderedSet\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor, SpatialDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "\n",
    "# import dataset as df\n",
    "dataDir = r\"D:\\Grid3\\ISS\\inputs\\HF Countries data.gdb\"\n",
    "country_code = \"CMR\" # This is used for writing files out\n",
    "country_prefix = \"CMR\" # This is used for input file name\n",
    "mfl = \"Formation_sanitaire_PEV_OMS_\"#\"\"MLHF_\" + country_prefix\n",
    "mfl_file = os.path.join(dataDir, mfl)\n",
    "\n",
    "COUNTRY = 'country'\n",
    "#COUNTRY_NAME = 'Democratic Republic of the Congo'\n",
    "COUNTRY_NAME = 'Cameroon'\n",
    "\n",
    "# current fields in mlhf\n",
    "full_hf_name = \"health_fac\"\n",
    "hf_type = \"type\"\n",
    "admin1=\"region\"\n",
    "admin2=\"district_n\"\n",
    "admin3=\"area_name\"\n",
    "# new fields \n",
    "# full_clean_name = \"facility_clean\"\n",
    "# short_name = \"facility_short\"\n",
    "# match_type = \"type_clean\"\n",
    "\n",
    "# hf_match_code = \"HF_Code\"\n",
    "\n",
    "# output columns\n",
    "FACILITY_NAME = \"health_fac\"\n",
    "CLEAN_NAME = \"facility_clean\" # clean name after some pre-cleaning\n",
    "CORRECT_NAME = 'corrected_name' # clean name after spelling correction\n",
    "CLEAN_NAME_FINAL = \"facility_short\" # final clean name after removing type information\n",
    "EXTRACT_TYPE = 'type_clean' # type information extracted\n",
    "SUB_TYPE = \"type_clean\" # type mapped to the type dictonary\n",
    "SCORE = 'score' # match score between 'type_extract' and 'sub_type'\n",
    "mfl_uuid=\"mfl_uuid\"\n",
    "hf_match_code = \"HF_Code\"\n",
    "\n",
    "\n",
    "# import type dictionary as TYPE_DICT\n",
    "path_to_type_dict = r\"D:\\Grid3\\ISS\\inputs\\spelling_type_dict\\cmr_type_dict_augmented_1130.csv\"\n",
    "TYPE_DICT = pd.read_csv(path_to_type_dict )\n",
    "\n",
    "# import spelling dictionary as SPELLING_DICT\n",
    "path_to_spelling_dict = r\"D:\\Grid3\\ISS\\inputs\\spelling_type_dict\\cmr_spelling_dict_052021.csv\"\n",
    "SPELLING_DICT = pd.read_csv(path_to_spelling_dict )\n",
    "\n",
    "#final output\n",
    "SAVE_PATH =r\"D:\\Grid3\\ISS\\processing\\MFL_by_country\\mfl_by_country.gdb\"\n",
    "OUT_FILE = country_code + \"_mfl\"\n",
    "SAVE_FILE = os.path.join(SAVE_PATH, OUT_FILE)\n",
    "SAVE_FILE_csv=os.path.join(\"\\\\\".join(SAVE_PATH.split(\"\\\\\")[:-1]), OUT_FILE, \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean(df, facility_name, clean_name):\n",
    "    \n",
    "    # replace NAs with empty string ''\n",
    "    df[facility_name] = df[facility_name].fillna('')\n",
    "     # remove accent marks\n",
    "    df[clean_name] = [unidecode.unidecode(n) for n in df[facility_name]]\n",
    "    df[clean_name]=df[clean_name]+\" \"\n",
    "    df[clean_name] = df[clean_name].str.replace(\" III \",\" 3 \")\\\n",
    "        .str.replace(\" II \",\" 2 \")\\\n",
    "        .str.replace(\" I \",\" 1 \")\\\n",
    "        .str.replace(\" Iii \",\" 3 \")\\\n",
    "        .str.replace(\" Ii \",\" 2 \")\\\n",
    "        .str.replace(\" IV \",\" 4 \")\\\n",
    "        .str.replace(\" Iv \",\" 4 \")\n",
    "    df[clean_name]  =df[clean_name].apply(lambda x: \" \".join(re.split('(\\d+)', x)))\n",
    "    df[clean_name] = df[clean_name].str.strip()\\\n",
    "            .str.title()\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.replace('.', ' ')\\\n",
    "            .str.replace(':', ' ')\\\n",
    "            .str.replace(\"'\", ' ')\\\n",
    "            .str.replace('\"', ' ')\\\n",
    "            .str.replace('[', ' ')\\\n",
    "            .str.replace(']', ' ')\\\n",
    "            .str.replace('[-_,/\\(\\);.]', ' ')\\\n",
    "            .str.replace('&', ' and ')\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\\\n",
    "            .str.replace('center ', 'centre ', case=False)\\\n",
    "            .str.replace('^st ', ' saint ', case=False)\\\n",
    "            .str.replace('cl ', 'clinique ', case=False)\\\n",
    "            .str.replace('Geral ', 'General ', case=False)\\\n",
    "            .str.replace('Hospitals ', 'Hopital ', case=False)\\\n",
    "            .str.replace('Hospital ', 'Hopital ', case=False)\\\n",
    "            .str.replace(\"Urban \", \"Urbain \", case=False)\\\n",
    "            .str.replace(\"Distrital \", \"District \", case=False)\\\n",
    "            .str.replace('^hosp | hosp | hosp$|^hosp$', ' Hopital ', case=False)\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\\\n",
    "            .str.replace(\" De \", \" de \")\\\n",
    "\n",
    "            #.str.replace('Clinique', 'Clinic', case=False)\\\n",
    "            #.str.replace('Polyclinique', 'Polyclinic', case=False)\\\n",
    "            #.str.replace('Dispensaire', 'Dispensary', case=False)\\\n",
    "            # .str.replace('HÃ´pital', 'Hospital', case=False)\\\n",
    "            #.str.replace('Hopital', 'Hospital', case=False)\\\n",
    "    \n",
    "    # replace NAs in clean_name with empty string ''\n",
    "    df[clean_name] = df[clean_name].fillna('')\n",
    "    \n",
    "    # change emptry string in facility_name back to NA\n",
    "    df[facility_name] = df[facility_name].replace('', np.nan)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `corrected_name`\n",
    "\n",
    "Makes correction to possible misspellings using the spelling dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(df, spelling_dict, country_col, clean_name, output_col):\n",
    "    corrected_results = pd.DataFrame()\n",
    "\n",
    "    for country_name in df[country_col].unique():\n",
    "        # obtain dataset for the country\n",
    "        df_ctr = df[df[country_col].str.upper()==country_name.upper()]\n",
    "    \n",
    "        spelling_dict_ctr = spelling_dict[spelling_dict['Country'].str.upper()==country_name.upper()]\n",
    "        words_to_correct = spelling_dict_ctr['Word'].unique()\n",
    "\n",
    "        df_ctr[output_col] = df_ctr[clean_name]\n",
    "        for word in words_to_correct:\n",
    "            misspellings = list(spelling_dict_ctr[(spelling_dict_ctr['Word']==word)]['Misspelling'])\n",
    "            for misspelling in misspellings:\n",
    "                df_ctr[output_col] = df_ctr[output_col]\\\n",
    "                .str.replace('|'.join(['^'+misspelling+' ', ' '+misspelling+' ',\n",
    "                                           ' '+misspelling+'$', '^'+misspelling+'$']), ' '+word+' ', case=False)\\\n",
    "                .str.strip().replace(\" De \",\" de \")\n",
    "    \n",
    "        # merge country results to all results\n",
    "        corrected_results = pd.concat([corrected_results, df_ctr])\n",
    "    # reset and drop index\n",
    "    corrected_results.reset_index(inplace=True, drop=True)\n",
    "                                                              \n",
    "    return corrected_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_code(df,clean_name, admin1, admin2,mfl_uuid):\n",
    "    \n",
    "    # replace NAs with empty string ''\n",
    "    df[\"temp_admin\"] = df[admin2].fillna('')\n",
    "     # remove accent marks\n",
    "    df[\"temp_admin\"] = [unidecode.unidecode(n) for n in df[\"temp_admin\"]]\n",
    "    df[\"temp_admin\"]=df[\"temp_admin\"]+\" \"\n",
    "    df[\"temp_admin\"] = df[\"temp_admin\"].str.replace(\" III \",\" 3 \")\\\n",
    "        .str.replace(\" II \",\" 2 \")\\\n",
    "        .str.replace(\" I \",\" 1 \")\\\n",
    "        .str.replace(\" Iii \",\" 3 \")\\\n",
    "        .str.replace(\" Ii \",\" 2 \")\\\n",
    "        .str.replace(\" IV \",\" 4 \")\\\n",
    "        .str.replace(\" Iv \",\" 4 \")\n",
    "    df[\"temp_admin\"]  =df[\"temp_admin\"].apply(lambda x: \" \".join(re.split('(\\d+)', x)))\n",
    "    df[\"temp_admin\"] = df[\"temp_admin\"].str.title()\\\n",
    "            .str.replace('[-_,/\\(\\);.]', ' ')\\\n",
    "            .str.replace('  ', ' ')\\\n",
    "            .str.strip()\n",
    "\n",
    "    # replace empty string '' with \"NA\"\n",
    "    df[\"temp_admin\"] = df[\"temp_admin\"].fillna('NA')\n",
    "    \n",
    "    # create hf_code  by combining hf name and admin2 (district)\n",
    "    df[hf_match_code] = df[clean_name]+\"_\"+df[\"temp_admin\"]\n",
    "    del df[\"temp_admin\"]\n",
    "    # create uuid for each facility\n",
    "    def my_random_string(string_length=10):\n",
    "        \"\"\"Returns a random string of length string_length.\"\"\"\n",
    "        random = str(uuid.uuid4()) # Convert UUID format to a Python string.\n",
    "        random = random.upper() # Make all characters uppercase.\n",
    "        random = random.replace(\"-\",\"\") # Remove the UUID '-'.\n",
    "        return random[0:string_length] # Return the random string.\n",
    "    # create uuid \n",
    "    df[mfl_uuid]=\"\"\n",
    "    df[\"unique_name\"]=df[clean_name]+\"_\"+df[admin1]+\"_\"+df[admin2]\n",
    "    for name in df[\"unique_name\"].unique():\n",
    "        df.loc[df[\"unique_name\"] == name, mfl_uuid] =my_random_string(string_length=10)\n",
    "    del df[\"unique_name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_type_info(df, type_dict, clean_name, clean_name_final, country_name):\n",
    "    # remove whitespace between abbreviations of length 2 or 3\n",
    "    # e.g. change C S to CS\n",
    "    \n",
    "    # convert country name to match country name in spelling and type dict\n",
    "    df['country_eng']= country_name\n",
    "    \n",
    "    # obtain abbreviations of length 2 or 3\n",
    "    tmp = type_dict[type_dict['Abbreviation'].str.len()<=3]['Abbreviation'].unique()\n",
    "    # sort by decreasing length\n",
    "    tmp = sorted(tmp, key=len, reverse=True)\n",
    "    # change it to the pattern '^c s ' or ' c s$'\n",
    "    tmp_dict = {}\n",
    "    for t in tmp:\n",
    "        tmp_dict[t] = ['^'+' '.join(list(t))+' ', ' '+' '.join(list(t))+'$']\n",
    "    # replace the pattern with 'cs'\n",
    "    for t in tmp:\n",
    "        pats = tmp_dict[t]\n",
    "        df[clean_name] = df[clean_name].str.replace(pats[0], t+' ',case=False)\\\n",
    "        .str.replace(pats[1], ' '+t, case=False)\n",
    "\n",
    "    # remove type information\n",
    "    df_grouped = df.groupby('country_eng')\n",
    "    res = pd.DataFrame()\n",
    "\n",
    "    for group_name, df_group in df_grouped:\n",
    "        # obtain the type dictionary for that country\n",
    "        tmp = type_dict[type_dict['Country'].str.upper()==group_name.upper()]\n",
    "\n",
    "        # facility types for that country\n",
    "        types = list(tmp['Type'])\n",
    "        type_keywords = set()\n",
    "        for t in types:\n",
    "            # add the full facility type \n",
    "            t = t.title()\n",
    "            type_keywords.add(t)                 \n",
    "\n",
    "            # add individual words as well\n",
    "            t = t.replace('/', ' ')\n",
    "            words = t.split(' ')\n",
    "            # skip words that have punctuation / numbers and have length <= 3 (e.g. de, (major))\n",
    "            words = [w for w in words if w.isalpha() and len(w)>3]\n",
    "            for w in words:\n",
    "                type_keywords.add(w)\n",
    "\n",
    "        # obtain the list of type keywords and sort in descending length\n",
    "        type_keywords = list(type_keywords)\n",
    "        type_keywords = sorted(type_keywords, key=lambda s: -len(s))\n",
    "\n",
    "        # abbreviations for that country\n",
    "        abbrevs = set(tmp['Abbreviation'])\n",
    "\n",
    "        abb_keywords = []\n",
    "        for abbrev in abbrevs:\n",
    "            # e.g. for CS, 4 patterns are considered: '^CS ', ' CS ', ' CS$', '^CS$'\n",
    "            abbrev = abbrev.title()\n",
    "            abb_keywords.extend(['^'+abbrev+'\\s', '\\s'+abbrev+'\\s', '\\s'+abbrev+'$',\n",
    "                                '^'+abbrev+'$'])\n",
    "\n",
    "        # obtain the list of abbreviation keywords and sort in descending length\n",
    "        abb_keywords = sorted(abb_keywords, key=lambda s: -len(s))  \n",
    "\n",
    "\n",
    "        # handle situations when type is 'Hospital District' in the type dictionary \n",
    "        # but name column has 'District Hospital' in ISS data\n",
    "        type_len_2 = [t for t in type_keywords if len(t.split())==2]\n",
    "        for t in type_len_2:\n",
    "            df_group[clean_name] = df_group[clean_name].str.title()\\\n",
    "            .str.replace(' '.join(t.split()[::-1]), t, case=False)\n",
    "\n",
    "        # remove type information using keywords generated above\n",
    "        # remove meaningless connecting words like de, do, da, du\n",
    "        df_group[clean_name_final] = df_group[clean_name].str.title()\\\n",
    "            .str.replace('|'.join(type_keywords), '')\\\n",
    "            .str.replace('|'.join(abb_keywords), ' ')\\\n",
    "            .str.strip()\\\n",
    "            .str.replace('^de | de | de$|^de$|^do | do | do$|^do$|^da | da | da$|^da$|^du | du | du$|^du$', \n",
    "                         ' ', case=False)\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\\\n",
    "            .str.title()\n",
    "        res = pd.concat([res, df_group])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_type(df, clean_name, clean_name_final, extract_type):\n",
    "    extract_types = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        name = row[clean_name].upper()\n",
    "        name_final = row[clean_name_final].upper()\n",
    "\n",
    "        # if clean_name_final is exactly the same as clean_name,\n",
    "        # this indicates no type information can be extracted, thus append NA\n",
    "        if name.upper() == name_final.upper():\n",
    "            extract_types.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            name = OrderedSet(name.split())\n",
    "            name_final = OrderedSet(name_final.split())\n",
    "            # find the difference between two names\n",
    "            diff = ' '.join(list(name.difference(name_final)))\n",
    "            extract_types.append(diff.strip())\n",
    "\n",
    "    # remove de, do, da, du at start or end of extract_type\n",
    "    # replace empty string with NA\n",
    "    df[extract_type] = extract_types\n",
    "    df[extract_type] = df[extract_type].str.strip()\\\n",
    "        .str.replace(\"  \", \" \")\\\n",
    "        .str.replace('^de |^do |^da |^du | du$| de$| do$| da$|^de$|^do$|^da$|^du$', '', case=False)\\\n",
    "        .str.replace('^de |^do |^da |^du | du$| de$| do$| da$|^de$|^do$|^da$|^du$', '', case=False)\\\n",
    "        .str.strip()\\\n",
    "        .str.title()\\\n",
    "        .str.replace(\" De \", \" de \")\\\n",
    "        .replace('',\"NA\")\n",
    "    df[clean_name]=df[clean_name].str.replace(' De ', \" de \")\n",
    "    df[clean_name_final].replace(\"\", np.nan, inplace=True)\n",
    "    df[clean_name_final].fillna(\"NA\", inplace=True)\n",
    "    df[extract_type].fillna(\"NA\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sub_type`\n",
    "\n",
    "Use `extract_type` to map the type information extracted from the name column to one of the types in the type dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_type(df, country, extract_type, sub_type, score, type_dict):\n",
    "    df_grouped = df.groupby(country)\n",
    "    res = pd.DataFrame()\n",
    "    for country_name in df[country].unique():\n",
    "        df_group = df[df[country]==country_name]\n",
    "        # obtain facility types and abbreviations for that country\n",
    "        tmp = type_dict[type_dict['Country'].str.upper()==country_name.upper()]\n",
    "        types, abbrevs = tmp['Type'], tmp['Abbreviation']\n",
    "        sub_types = []\n",
    "        scores = []\n",
    "\n",
    "        for idx, row in df_group.iterrows():\n",
    "            # if extract_type is NA, just append NA\n",
    "            if not isinstance(row[extract_type],str):\n",
    "                sub_types.append(np.nan)\n",
    "                scores.append(np.nan)\n",
    "\n",
    "            # find best match\n",
    "            else:\n",
    "                match, match_score = process.extractOne(row[extract_type], list(types)+list(abbrevs), scorer = fuzz.token_sort_ratio)\n",
    "            \n",
    "                scores.append(match_score)\n",
    "                # if best match is abbreviation, map it to the corresponding type\n",
    "                if match in list(abbrevs):\n",
    "                    match_type = tmp[tmp['Abbreviation']==match]['Type'].iloc[0]\n",
    "                    sub_types.append(match_type)\n",
    "                else:\n",
    "                    sub_types.append(match) \n",
    "        df_group[sub_type] = sub_types\n",
    "        df_group[score] = scores\n",
    "        res = pd.concat([res, df_group])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:19: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:23: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:24: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:38: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:74: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:75: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:78: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "D66140B0-0277-45FF-AE18-A787BE3D4AAF:26: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Number of facility in master facility list:\n",
      "5458\n",
      "\n",
      ">>> Summary by type of facilities:\n",
      "Csi                   1624\n",
      "Priv??                 772\n",
      "Cs                     540\n",
      "Ihc                    255\n",
      "Cma                    219\n",
      "                      ... \n",
      "Privee                   1\n",
      "Hopital Catholique       1\n",
      "Hospitalier              1\n",
      "Hd de District           1\n",
      "Regional Health          1\n",
      "Name: type_extract, Length: 211, dtype: int64\n",
      "\n",
      ">>> Number of facility that duplicated in MFL:\n",
      "339\n",
      "\n",
      ">>> Word frequency in facility name:\n"
     ]
    }
   ],
   "source": [
    "# read input dataset as spatial dataframe\n",
    "sdf=pd.DataFrame.spatial.from_featureclass(mfl_file)\n",
    "# pre-clean hf name\n",
    "preclean(df=sdf, facility_name=full_hf_name, clean_name = CLEAN_NAME)\n",
    "\n",
    "corrected_results = correct_spelling(sdf, spelling_dict=SPELLING_DICT, country_col=COUNTRY, \n",
    "                                     clean_name = CLEAN_NAME, output_col=CLEAN_NAME_FINAL)\n",
    "# create hf code\n",
    "create_hf_code(corrected_results,CLEAN_NAME_FINAL,admin1, admin2,mfl_uuid)\n",
    "# # # separate type from name\n",
    "clean_sdf = remove_type_info(df=corrected_results, type_dict=TYPE_DICT, clean_name=CLEAN_NAME, \n",
    "                             clean_name_final=CLEAN_NAME_FINAL,country_name=COUNTRY_NAME)\n",
    "# # remove type from hf name\n",
    "extract_type(df=clean_sdf, clean_name=CLEAN_NAME, \n",
    "              clean_name_final=CLEAN_NAME_FINAL, extract_type=EXTRACT_TYPE)\n",
    "\n",
    "# final check for name and type\n",
    "\n",
    "for index, row in clean_sdf.iterrows():\n",
    "    if clean_sdf.at[index, EXTRACT_TYPE]==\"NA\":\n",
    "        \n",
    "        get_type=unidecode.unidecode(clean_sdf.at[index, hf_type])\n",
    "\n",
    "        clean_sdf.at[index,EXTRACT_TYPE ]=get_type\n",
    "    if clean_sdf.at[index,CLEAN_NAME_FINAL ]==\"NA\":\n",
    "        clean_sdf.at[index,CLEAN_NAME_FINAL]=clean_sdf.at[index, EXTRACT_TYPE]\n",
    "\n",
    "# check if facility duplicated\n",
    "facility_uuid=clean_sdf[mfl_uuid].tolist()\n",
    "dup_facility=[item for item, count in collections.Counter(facility_uuid).items() if count > 1]\n",
    "\n",
    "clean_sdf[\"duplicate_uuid\"]=0\n",
    "for index, row in clean_sdf.iterrows():\n",
    "    if clean_sdf.at[index, mfl_uuid ] in dup_facility :\n",
    "        clean_sdf.at[index, \"duplicate_uuid\"]=1\n",
    "  \n",
    "##===================Result=========================##\n",
    "print(\">>> Number of facility in master facility list:\")\n",
    "print(clean_sdf.shape[0])\n",
    "print()\n",
    "print(\">>> Summary by type of facilities:\")\n",
    "print(clean_sdf[EXTRACT_TYPE].value_counts())\n",
    "print()\n",
    "\n",
    "print(\">>> Number of facility that duplicated in MFL:\")\n",
    "unique_facility=len(clean_sdf[mfl_uuid].unique())\n",
    "print(clean_sdf.shape[0]-unique_facility)\n",
    "print()\n",
    "print(\">>> Word frequency in facility name:\")\n",
    "clean_sdf[CLEAN_NAME_FINAL].str.split(expand=True).stack().value_counts()[:20]\n",
    "# export result to spatial layer\n",
    "clean_sdf[\"adm1_name\"]=clean_sdf[admin1]\n",
    "clean_sdf[\"adm2_name\"]=clean_sdf[admin2]\n",
    "clean_sdf[\"adm3_name\"]=clean_sdf[admin3]\n",
    "clean_sdf.spatial.to_featureclass(SAVE_FILE)\n",
    "clean_sdf.to_csv(SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## get count of facility type\n",
    "clean_sdf=pd.DataFrame.spatial.from_featureclass(\"CMR_mfl\")\n",
    "type_count=clean_sdf['facility_short'].value_counts()\n",
    "type_count_df=pd.DataFrame(type_count).reset_index().rename({\"type_clean\":\"frequancy\", \"index\":\"type\"}, axis=1)\n",
    "#type_count_df.to_csv(\"\\\\\".join(SAVE_PATH.split(\"\\\\\")[:-1])+\"\\\\CMR_MFL_type_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=clean_sdf[\"facility_short\"].str.split(expand=True).stack().value_counts()\n",
    "word_count_df=pd.DataFrame(word_count).reset_index().rename({0:\"frequancy\", \"index\":\"abrv\"}, axis=1)\n",
    "word_count_df=word_count_df[(word_count_df[\"abrv\"].str.len()<=4)&(word_count_df[\"frequancy\"]>=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
