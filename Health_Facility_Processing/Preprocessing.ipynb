{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook performs:\n",
    "+ Data cleaning on the facility name column which includes removal of punctuation and removal of facility type information contained in the facility name column.\n",
    "+ The removed type information in the facility name by using a facility type dictionary.\n",
    "+ Complite health facility types.\n",
    "+ Check if points located in the right admin boundary based on provided admin boundary.\n",
    "+ Check if points located on a settlement.\n",
    "+ Check if points are overlap (identical lat/long).\n",
    "+ Match Master Facility List (MFL).\n",
    "\n",
    "### Input:\n",
    "#### Spatial layers:\n",
    "+ Health facilities locations\n",
    "+ Settlement extent layer.BUA, SSA and Hamlets needs to be merge into a layer\n",
    "+ Admin 1 boundary \n",
    "+ Admin 2 boundary\n",
    "#### Tables\n",
    "+ Spelling dictionary to fix spellings differences in the facility types\n",
    "+ Type dictionary to standardize facility types\n",
    "+ Master Facility List (MFL). Facility names and types need to be seperated into two columns\n",
    "\n",
    "#### User Inputs:\n",
    " User needs to specify these inputs before running the script\n",
    "+ root_dir: Output workspace\n",
    "+ input_data: name of the input layer\n",
    "+ today_date: Date. it will be used in output directory and gdb names\n",
    "+ admin_name: It will be used as name convention in the output layers\n",
    "+ point_poi_type: It will be used as name convention in the output layers\n",
    "+ points_source: It will be used as name convention in the output layers\n",
    "+ path_to_type_dict: Path to facility type dictionary\n",
    "+ path_to_spelling_dict: Path to spelling dictionary\n",
    "+ ADMIN1_BNDRY : Admin 1 column in the admin 1 boundary layer\n",
    "+ ADMIN2_BNDRY : Admin 2 column in the admin 2 boundary layer\n",
    "+ COUNTRY: Country name. It is used for getting spelling dictionary and type dictionary for each country\n",
    "+ ADMIN1=Admin 1 name in the health facility layer\n",
    "+ ADMIN2=Admin 2 name in the health facility layer\n",
    "+ ADMIN3=Admin 3 name in the health facility layer\n",
    "+ FACILITY_NAME = Facility name\n",
    "+ CLEAN_NAME =clean facility name after pre-cleaning.\n",
    "+ CORRECT_NAME = corrected facility name after misspelling  correction.\n",
    "+ CLEAN_NAME_FINAL = final clean name after removing type information.\n",
    "+ EXTRACT_TYPE = type information extracted, i.e. the difference between CORRECTED_NAME and clean_name_final.\n",
    "+ SUB_TYPE = facility type defined in the type dictonary, obtained by mapping EXTRACT_TYPE to type dictionary\n",
    "+ SCORE = match score between EXTRACT_TYPE and SUB_TYPE (scale 0-100), can be used to filter perfect-match results only.\n",
    "+ dist_to_sett_threshold=Search distance to check points against settlement extents \n",
    "+ dist_to_check_overlap=Search distance to check points for overlaps\n",
    "+ MFL_ADMIN1=Admin 1 column in the MFL\n",
    "+ MFL_ADMIN2=Admin 2 column in the MFL\n",
    "+ MFL_ADMIN3=Admin 3 column in the MFL\n",
    "+ MFL_FACE_NAME=Facility name column in the MFL\n",
    "+ MFL_FACE_TYPE=Facility type column in the MFL\n",
    "+ MFL_ID=Facility unique id column in the MFL\n",
    "\n",
    "\n",
    "### Output:\n",
    " A point layer with these columns :\n",
    "- org_name: Original facility name. It is combination of hf, hf1 and hf2 columns\n",
    "- duration_bins: Cassification of interview time in minutes\n",
    "- clean_name: Clean facility name after pre-cleaning.\n",
    "- corrected_name: Corrected facility name after misspelling  correction.\n",
    "- clean_name_final: Final clean name after removing type information.\n",
    "- extract_type: Type information extracted, i.e. the difference between CORRECTED_NAME and clean_name_final.\n",
    "- type: Tacility type defined in the type dictonary, obtained by mapping EXTRACT_TYPE to type dictionary\n",
    "- score: Match score between EXTRACT_TYPE and SUB_TYPE (scale 0-100), can be used to filter perfect-match results only.\n",
    "- admin1_bdry_match: Indicates if points lacated in the right admin boundary (e.g province_bdry_match)\n",
    "- admin2_bdry_match: Indicates if points lacated in the right admin boundary (e.g h_zone1_bdry_match)\n",
    "- sett_type: Settlement type that a point are located (bua, ssa, hamlets).Points that far from a settlement more than 250 meters classified as \"out of a settelement\"\n",
    "- is_overlaps: Indicates if points have identical lat/long\n",
    "- admin1_2: Updated admin 1 names after matching to  MFL (e.g province_2)\n",
    "- admin1_2_is_match: Indicates if admin 1 names match to MFL (e.g province_2_is_match)\n",
    "- admin2_2: Updated admin 2 names after matching to  MFL (e.g h_zone2)\n",
    "- admin2_2_is_match: Indicates if admin 2 names match to MFL (e.g h_zone2_is_match)\n",
    "- admin3_2: Updated admin 3 names after matching to  MFL (e.g h_area2)\n",
    "- admin3_2_is_match: Indicates if admin 3 names match to MFL (e.g h_area2_is_match)\n",
    "- face_name2: Updated  facility names after matching to  MFL\n",
    "- face_name2_is_match: Indicates if admin 3 names match to MFL (e.g h_area2_is_match)\n",
    "- mfl_admin2= Admin 2 name from MFL. Only if facility names match to MFL (e.g mfl_health_zone)\n",
    "- mfl_admin3= Admin 3 name from MFL. Only if facility names match to MFL (e.g mfl_health_area)\n",
    "- mfl_unique_id= Unique id for each facility  from MFL. Only if facility names match to MFL (e.g fosa_uid). Some facilities with the same name but different types may cross match. These cases are flagged as \"duplicate_match\"\n",
    "- mfl_unique_id= Facility type from MFL. Only if facility names match to MFL (e.g mfl_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hengin\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-clone\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#import geopandas as gpd\n",
    "#import fiona\n",
    "import os\n",
    "import unidecode\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from ordered_set import OrderedSet\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor, SpatialDataFrame\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster(sdf,distance, min_point,cluster_variable):\n",
    "    '''checks overlappes and distance between points based on specified distance\n",
    "    ----------------------------------------------------------------------------\n",
    "    inputs:\n",
    "        PoI: point of intereset layer\n",
    "        distance= distance (in meters) threshold to check clusters\n",
    "        min_point= mininun point count to check for a cluster\n",
    "        crf= a projected coordinate system code as a string\n",
    "            (102022 >> Africa equal area projection. Change if needed)\n",
    "    output:\n",
    "        A new column will be added to the PoI layer\n",
    "        cluster_id: Points that are in the same clusters will have the same id value.\n",
    "        if point that has cluster_id more than 99999 means that the point is not in a cluster.  \n",
    "    '''\n",
    "    ##===========================================================================##\n",
    "    #output_name=os.path.join(output_gdb,PoI)\n",
    "    cluster_variable=\"cluster_id_\"+str(distance)+\"m\"\n",
    "  \n",
    "     ##----------------------------------------------------------##\n",
    "   \n",
    "    if  cluster_variable in sdf.columns:\n",
    "        sdf.drop(cluster_variable, axis=1, inplace=True)\n",
    "    points_coord=sdf[['x_coord_m','y_coord_m']]\n",
    "    \n",
    "    db     = DBSCAN(eps=distance, min_samples=min_point).fit(points_coord)\n",
    "    labels = db.labels_ #labels of the found clusters, points that out of a cluster coded as -1\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0) #number of clusters\n",
    "    labels_list=list(labels)\n",
    "    # attached cluster id to df\n",
    "    points_coord[cluster_variable]=np.nan\n",
    "    for i in range(len(labels_list)):\n",
    "        points_coord.loc[i,cluster_variable]=int(labels_list[i])\n",
    "    points_coord_=points_coord[[cluster_variable]]    \n",
    "    sdf_=sdf.merge(points_coord_, left_index=True, right_index=True)\n",
    "    # delete temp. columns\n",
    "    sdf_.drop(['x_coord_m','y_coord_m'], axis=1, inplace=True)\n",
    "    \n",
    "    # points that out of selected distance cluster are coded -1 by DBSCAN function\\\n",
    "    # section below creates unique \"cluster_id\" for the points that coded as -1\\\n",
    "    # if cluster_id is more than 9999, it means that these cluster are a single point cluster\n",
    "    i=999999\n",
    "    for idx, row in sdf_.iterrows():\n",
    "              \n",
    "        if row[cluster_variable]==-1.0:\n",
    "        \n",
    "            sdf_.loc[idx,cluster_variable]=i\n",
    "            i=i+1\n",
    "    #arcpy.DeleteFeatures_management(output_name)\n",
    "    #sdf_.spatial.to_featureclass(output_name)\n",
    "    return sdf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `clean_name`\n",
    "\n",
    "Pre-cleaning on facility name:\n",
    "\n",
    "- remove punctuations, change '&' to 'and'\n",
    "- correct spelling of common words for consistency\n",
    "- replace double whitespaces with one and strip extra whitespaces\n",
    "- remove accent marks\n",
    "\n",
    "Note: NA values in facility name column is replaced with empty string '' first and then converted back to NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean(df, facility_name, clean_name, country_col):\n",
    "    \n",
    "    # replace NAs with empty string ''\n",
    "    df[facility_name] = df[facility_name].fillna('')\n",
    "     # remove accent marks\n",
    "    df[clean_name] = [unidecode.unidecode(n) for n in df[facility_name]]\n",
    "    df[clean_name]=df[clean_name]+\" \"\n",
    "    df[clean_name] = df[clean_name].str.replace(\" III \",\" 3 \")\\\n",
    "        .str.replace(\" II \",\" 2 \")\\\n",
    "        .str.replace(\" I \",\" 1 \")\\\n",
    "        .str.replace(\" Iii \",\" 3 \")\\\n",
    "        .str.replace(\" Ii \",\" 2 \")\\\n",
    "        .str.replace(\" IV \",\" 4 \")\\\n",
    "        .str.replace(\" Iv \",\" 4 \")\n",
    "    df[clean_name]  =df[clean_name].apply(lambda x: \" \".join(re.split('(\\d+)', x)))\n",
    "    df[clean_name] = df[clean_name].str.strip()\\\n",
    "            .str.title()\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.replace('.', '')\\\n",
    "            .str.replace(':', '')\\\n",
    "            .str.replace(\"'\", '')\\\n",
    "            .str.replace('\"', ' ')\\\n",
    "            .str.replace('[', '')\\\n",
    "            .str.replace(']', '')\\\n",
    "            .str.replace('+', '')\\\n",
    "            .str.replace('*', '')\\\n",
    "            .str.replace('[-_,/\\(\\);]', '')\\\n",
    "            .str.replace('&', ' and ')\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\\\n",
    "            .str.replace('center ', 'centre ', case=False)\\\n",
    "            .str.replace('^st ', ' saint ', case=False)\\\n",
    "            .str.replace(' st ', ' saint ', case=False)\\\n",
    "            .str.replace('cl ', 'clinique ', case=False)\\\n",
    "            .str.replace('Geral ', 'General ', case=False)\\\n",
    "            .str.replace('Hospitals ', 'Hopital ', case=False)\\\n",
    "            .str.replace('Hospital ', 'Hopital ', case=False)\\\n",
    "            .str.replace(\"Urban \", \"Urbain \", case=False)\\\n",
    "            .str.replace(\"Distrital \", \"District \", case=False)\\\n",
    "            .str.replace('^hosp | hosp | hosp$|^hosp$', ' Hopital ', case=False)\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\\\n",
    "            .str.replace(\" De \", \" de \")\\\n",
    "\n",
    "            #.str.replace('Clinique', 'Clinic', case=False)\\\n",
    "            #.str.replace('Polyclinique', 'Polyclinic', case=False)\\\n",
    "            #.str.replace('Dispensaire', 'Dispensary', case=False)\\\n",
    "            # .str.replace('HÃ´pital', 'Hospital', case=False)\\\n",
    "            #.str.replace('Hopital', 'Hospital', case=False)\\\n",
    "    \n",
    "    # replace NAs in clean_name with empty string ''\n",
    "    df[clean_name] = df[clean_name].fillna('')\n",
    "    \n",
    "    # change emptry string in facility_name back to NA\n",
    "    df[facility_name] = df[facility_name].replace('', np.nan)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Admin names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean_admin(df,admin_var, out_admin_var):\n",
    "    \n",
    "    # replace NAs with empty string ''\n",
    "    df[out_admin_var] = df[admin_var].fillna('')\n",
    "     # remove accent marks\n",
    "    df[out_admin_var] = [unidecode.unidecode(n) for n in df[out_admin_var]]\n",
    "    df[out_admin_var]=df[out_admin_var]+\" \"\n",
    "    df[out_admin_var] = df[out_admin_var].str.replace(\" III \",\" 3 \")\\\n",
    "        .str.replace(\" II \",\" 2 \")\\\n",
    "        .str.replace(\" I \",\" 1 \")\\\n",
    "        .str.replace(\" Iii \",\" 3 \")\\\n",
    "        .str.replace(\" Ii \",\" 2 \")\\\n",
    "        .str.replace(\" IV \",\" 4 \")\\\n",
    "        .str.replace(\" Iv \",\" 4 \")\n",
    "    df[out_admin_var]  =df[out_admin_var].apply(lambda x: \" \".join(re.split('(\\d+)', x)))\n",
    "    df[out_admin_var] = df[out_admin_var].str.title()\\\n",
    "            .str.replace('[-_,/\\(\\);.]', ' ')\\\n",
    "            .str.replace(\"AS \",\"\", case=False)\\\n",
    "            .str.replace(\"ZS \",\"\", case=False)\\\n",
    "            .str.replace('  ', ' ')\\\n",
    "            .str.strip()\\\n",
    "\n",
    "    # replace NAs in admin_var with empty string ''\n",
    "    df[out_admin_var] = df[out_admin_var].fillna('')\n",
    "    \n",
    "    # change emptry string in facility_name back to NA\n",
    "    df[out_admin_var] = df[out_admin_var].replace('', np.nan)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `corrected_name`\n",
    "\n",
    "Makes correction to possible misspellings using the spelling dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(df, spelling_dict, country_col, clean_name, output_col):\n",
    "    corrected_results = pd.DataFrame()\n",
    "\n",
    "    for country_name in df[country_col].unique():\n",
    "        # obtain dataset for the country\n",
    "        df_ctr = df[df[country_col].str.upper()==country_name.upper()]\n",
    "    \n",
    "        spelling_dict_ctr = spelling_dict[spelling_dict['Country'].str.upper()==country_name.upper()]\n",
    "        words_to_correct = spelling_dict_ctr['Word'].unique()\n",
    "\n",
    "        df_ctr[output_col] = df_ctr[clean_name]\n",
    "        for word in words_to_correct:\n",
    "            misspellings = list(spelling_dict_ctr[(spelling_dict_ctr['Word']==word)]['Misspelling'])\n",
    "            for misspelling in misspellings:\n",
    "                df_ctr[output_col] = df_ctr[output_col]\\\n",
    "                .str.replace('|'.join(['^'+misspelling+' ', ' '+misspelling+' ',\n",
    "                                           ' '+misspelling+'$', '^'+misspelling+'$']), ' '+word+' ', case=False)\\\n",
    "                .str.strip().replace(\" De \",\" de \")\n",
    "    \n",
    "        # merge country results to all results\n",
    "        corrected_results = pd.concat([corrected_results, df_ctr])\n",
    "    # reset and drop index\n",
    "    corrected_results.reset_index(inplace=True, drop=True)\n",
    "                                                              \n",
    "    return corrected_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `clean_name_final`\n",
    "\n",
    "Use facility type and abbreviations in the type dictionary as keywords and remove type information from `corrected_name` to create the `clean_name_final` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_type_info(df, type_dict, clean_name, clean_name_final, country):\n",
    "    # remove whitespace between abbreviations of length 2 or 3\n",
    "    # e.g. change C S to CS\n",
    "    \n",
    "    # obtain abbreviations of length 2 or 3\n",
    "    tmp = type_dict[type_dict['Abbreviation'].str.len()<=3]['Abbreviation'].unique()\n",
    "    # sort by decreasing length\n",
    "    tmp = sorted(tmp, key=len, reverse=True)\n",
    "    # change it to the pattern '^c s ' or ' c s$'\n",
    "    tmp_dict = {}\n",
    "    for t in tmp:\n",
    "        tmp_dict[t] = ['^'+' '.join(list(t))+' ', ' '+' '.join(list(t))+'$']\n",
    "    # replace the pattern with 'cs'\n",
    "    for t in tmp:\n",
    "        pats = tmp_dict[t]\n",
    "        df[clean_name] = df[clean_name].str.replace(pats[0], t+' ',case=False)\\\n",
    "        .str.replace(pats[1], ' '+t, case=False)\n",
    "    # remove type information\n",
    "    df_grouped = df.groupby(country)\n",
    "    res = pd.DataFrame()\n",
    "\n",
    "    for group_name, df_group in df_grouped:\n",
    "        # obtain the type dictionary for that country\n",
    "        tmp = type_dict[type_dict['Country'].str.upper()==group_name.upper()]\n",
    "\n",
    "        # facility types for that country\n",
    "        types = list(tmp['Type'])\n",
    "        type_keywords = set()\n",
    "        for t in types:\n",
    "            # add the full facility type \n",
    "            t = t.title()\n",
    "            type_keywords.add(t)                 \n",
    "\n",
    "            # add individual words as well\n",
    "            t = t.replace('/', ' ')\n",
    "            words = t.split(' ')\n",
    "            # skip words that have punctuation / numbers and have length <= 3 (e.g. de, (major))\n",
    "            words = [w for w in words if w.isalpha() and len(w)>3]\n",
    "            for w in words:\n",
    "                type_keywords.add(w)\n",
    "\n",
    "        # obtain the list of type keywords and sort in descending length\n",
    "        type_keywords = list(type_keywords)\n",
    "        type_keywords = sorted(type_keywords, key=lambda s: -len(s))\n",
    "\n",
    "        # abbreviations for that country\n",
    "        abbrevs = set(tmp['Abbreviation'])\n",
    "\n",
    "        abb_keywords = []\n",
    "        for abbrev in abbrevs:\n",
    "            # e.g. for CS, 4 patterns are considered: '^CS ', ' CS ', ' CS$', '^CS$'\n",
    "            abbrev = abbrev.title()\n",
    "            abb_keywords.extend(['^'+abbrev+'\\s', '\\s'+abbrev+'\\s', '\\s'+abbrev+'$',\n",
    "                                '^'+abbrev+'$'])\n",
    "\n",
    "        # obtain the list of abbreviation keywords and sort in descending length\n",
    "        abb_keywords = sorted(abb_keywords, key=lambda s: -len(s))  \n",
    "\n",
    "\n",
    "        # handle situations when type is 'Hospital District' in the type dictionary \n",
    "        # but name column has 'District Hospital' in ISS data\n",
    "        type_len_2 = [t for t in type_keywords if len(t.split())==2]\n",
    "        for t in type_len_2:\n",
    "            df_group[clean_name] = df_group[clean_name].str.title()\\\n",
    "            .str.replace(' '.join(t.split()[::-1]), t, case=False)\n",
    "\n",
    "        # remove type information using keywords generated above\n",
    "        # remove meaningless connecting words like de, do, da, du\n",
    "        df_group[clean_name_final] = df_group[clean_name].str.title()\\\n",
    "            .str.replace('|'.join(type_keywords), '')\\\n",
    "            .str.replace('|'.join(abb_keywords), ' ')\\\n",
    "            .str.strip()\\\n",
    "            .str.replace('^de | de | de$|^de$|^do | do | do$|^do$|^da | da | da$|^da$|^du | du | du$|^du$', \n",
    "                         ' ', case=False)\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\\\n",
    "            .str.title()\n",
    "        res = pd.concat([res, df_group])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `extract_type`\n",
    "\n",
    "Extract facility type information by removing `clean_name_final` from `corrected_name`.\n",
    "\n",
    "Note: empty string '' in `clean_name_final` and `corrected_name` are converted back to NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_type(df, clean_name, clean_name_final, extract_type):\n",
    "    extract_types = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        name = row[clean_name].upper()\n",
    "        name_final = row[clean_name_final].upper()\n",
    "\n",
    "        # if clean_name_final is exactly the same as clean_name,\n",
    "        # this indicates no type information can be extracted, thus append NA\n",
    "        if name.upper() == name_final.upper():\n",
    "            extract_types.append(np.nan)\n",
    "\n",
    "        else:\n",
    "            name = OrderedSet(name.split())\n",
    "            name_final = OrderedSet(name_final.split())\n",
    "            # find the difference between two names\n",
    "            diff = ' '.join(list(name.difference(name_final)))\n",
    "            extract_types.append(diff.strip())\n",
    "\n",
    "    # remove de, do, da, du at start or end of extract_type\n",
    "    # replace empty string with NA\n",
    "    df[extract_type] = extract_types\n",
    "    df[extract_type] = df[extract_type].str.strip()\\\n",
    "        .str.replace(\"  \", \" \")\\\n",
    "        .str.replace('^de |^do |^da |^du | du$| de$| do$| da$|^de$|^do$|^da$|^du$', '', case=False)\\\n",
    "        .str.replace('^de |^do |^da |^du | du$| de$| do$| da$|^de$|^do$|^da$|^du$', '', case=False)\\\n",
    "        .str.strip()\\\n",
    "        .str.title()\\\n",
    "        .replace('',np.nan)\n",
    "    # replace empty string with NA\n",
    "    df[clean_name].replace('', np.nan, inplace=True)\n",
    "    df[clean_name_final].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sub_type`\n",
    "\n",
    "Use `extract_type` to map the type information extracted from the name column to one of the types in the type dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_type(df, country, extract_type, sub_type, score, type_dict):\n",
    "    df_grouped = df.groupby(country)\n",
    "    res = pd.DataFrame()\n",
    "    for country_name in df[country].unique():\n",
    "        df_group = df[df[country]==country_name]\n",
    "        # obtain facility types and abbreviations for that country\n",
    "        tmp = type_dict[type_dict['Country'].str.upper()==country_name.upper()]\n",
    "        types, abbrevs = tmp['Type'], tmp['Abbreviation']\n",
    "        sub_types = []\n",
    "        scores = []\n",
    "\n",
    "        for idx, row in df_group.iterrows():\n",
    "            # if extract_type is NA, just append NA\n",
    "            if not isinstance(row[extract_type],str):\n",
    "                sub_types.append(np.nan)\n",
    "                scores.append(np.nan)\n",
    "\n",
    "            # find best match\n",
    "            else:\n",
    "                match, match_score = process.extractOne(row[extract_type], list(types)+list(abbrevs), scorer = fuzz.token_sort_ratio)\n",
    "            \n",
    "                scores.append(match_score)\n",
    "                # if best match is abbreviation, map it to the corresponding type\n",
    "                if match in list(abbrevs):\n",
    "                    match_type = tmp[tmp['Abbreviation']==match]['Type'].iloc[0]\n",
    "                    sub_types.append(match_type)\n",
    "                else:\n",
    "                    sub_types.append(match) \n",
    "        df_group[sub_type] = sub_types\n",
    "        df_group[score] = scores\n",
    "        res = pd.concat([res, df_group])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complite types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complite_types(df):\n",
    "    df_group=df.groupby([ADMIN2, ADMIN3,CLEAN_NAME_FINAL,\"cluster_id_1000m\" ])\n",
    "    for i, _group in df_group:\n",
    "       \n",
    "        _group[SUB_TYPE]=_group[SUB_TYPE].fillna(\"xxx\")\n",
    "        get_types=list(_group[SUB_TYPE].unique())\n",
    "        \n",
    "        if len(get_types)==2 and \"xxx\" in get_types :\n",
    "            get_types.remove(\"xxx\")  \n",
    "            df.loc[(df[ADMIN2]==i[0])& (df[ADMIN3]==i[1])\\\n",
    "            &(df[CLEAN_NAME_FINAL]==i[2])&(df.cluster_id_1000m==i[3]), SUB_TYPE]=get_types*_group.shape[0]\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check by admin boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_by_admin(PoI,admin_var_from_poi, admin_boundary,admin_var_from_admin):\n",
    "    ##================================================================================##\n",
    "    # admin column from admin boundary in PoI layer\n",
    "    new_admin_col=admin_var_from_admin+\"_bdry\"\n",
    "    # result of match\n",
    "    check_var=admin_var_from_poi+\"_bdryMatch\"\n",
    "    #---------------------------------------------------------------------------------##\n",
    "    #get admin id that point located\n",
    "    arcpy.Near_analysis(PoI, admin_boundary)\n",
    "    # convert point layer to sdf\n",
    "    sdf_poi=pd.DataFrame.spatial.from_featureclass(PoI)\n",
    "    # clean admin layer to be matched\n",
    "    sdf_poi=preclean_admin(sdf_poi,admin_var_from_poi, \"poi_admin\")\n",
    "    # conver admin boundry to sdf\n",
    "    sdf_admin=pd.DataFrame.spatial.from_featureclass( admin_boundary)[[\"OBJECTID\", admin_var_from_admin]]\n",
    "    # clean admin name from boundary\n",
    "    sdf_admin=preclean_admin(sdf_admin,admin_var_from_admin, \"bdry_admin\")\n",
    "    sdf_admin.rename({ admin_var_from_admin:new_admin_col,\"OBJECTID\":\"Match_code\"}, axis=1, inplace=True)\n",
    "    # merge point layer with boundary layer attribute\n",
    "    sdf_merge=sdf_poi.merge(sdf_admin, how=\"left\", left_on=\"NEAR_FID\", right_on=\"Match_code\")\n",
    "    sdf_merge[check_var]=\"\"\n",
    "  \n",
    "    # match admin names between point layer and boundary layer\n",
    "    for index,  row in sdf_merge.iterrows():\n",
    "        score=fuzz.ratio(row[\"poi_admin\"], row[\"bdry_admin\"])\n",
    "        if score >=85:\n",
    "            sdf_merge.loc[index,check_var]=\"YES\"\n",
    "        else:\n",
    "            sdf_merge.loc[index,check_var]=\"NO\"\n",
    "    sdf_merge.drop([\"Match_code\",\"NEAR_FID\",\"NEAR_DIST\",\"poi_admin\",\"bdry_admin\",new_admin_col],axis=1, inplace=True)\n",
    "    arcpy.DeleteFeatures_management(PoI)\n",
    "    sdf_merge.spatial.to_featureclass(PoI)                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check by settlement types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_by_settlement(PoI, settExtent, dist_to_sett_threshold):\n",
    "    arcpy.Near_analysis(PoI, settExtent, dist_to_sett_threshold)\n",
    "    sdf_poi=pd.DataFrame.spatial.from_featureclass(PoI)   \n",
    "    sdf_sett=pd.DataFrame.spatial.from_featureclass( settExtent)[[\"OBJECTID\",\"type\"]]\n",
    "    sdf_sett=sdf_sett.rename({ \"type\":\"sett_type\",\"OBJECTID\":\"Match_code\"}, axis=1)\n",
    "    sdf_merge=sdf_poi.merge(sdf_sett, how=\"left\", left_on=\"NEAR_FID\", right_on=\"Match_code\")\n",
    "    sdf_merge[\"sett_type\"].fillna(\"Out of a settlement\", inplace=True)\n",
    "    sdf_merge.drop([\"Match_code\",\"NEAR_FID\",\"NEAR_DIST\"],axis=1, inplace=True)\n",
    "    arcpy.DeleteFeatures_management(PoI)\n",
    "    sdf_merge.spatial.to_featureclass(PoI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlaps(fc,check_dist_m):\n",
    "    out_table= r\"C:\\Users\\hengin\\Documents\\overlap.csv\"\n",
    "    arcpy.management.FindIdentical(fc, out_table, \"Shape\", dist_to_check_overlap, 0, \"ONLY_DUPLICATES\")\n",
    "    sdf=pd.DataFrame.spatial.from_featureclass(fc)\n",
    "    identical_df=pd.read_csv(out_table)[[\"IN_FID\"]]\n",
    "    identical_df['is_overlap']=\"YES\" \n",
    "    sdf_merge= sdf.merge(identical_df, how=\"left\", left_on=\"OBJECTID\", right_on=\"IN_FID\") \n",
    "    sdf_merge['is_overlap'].fillna(\"NO\",inplace=True) \n",
    "    sdf_merge.drop(\"IN_FID\", axis=1, inplace=True)\n",
    "    arcpy.DeleteFeatures_management(fc)\n",
    "    sdf_merge.spatial.to_featureclass(fc)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country iso code\n",
    "country_code=\"DRC\" #  options country, province, districts, health zone, health area, depent on input data\n",
    "\n",
    "\n",
    "# main directory that have input layers.Output will be saved in this directory\n",
    "root_dir=r\"D:\\Grid3\\ISS\\processing\"\n",
    "#input layer\n",
    "#input_data=r\"D:\\Grid3\\ISS\\processing\\DRC_WHO_health_facilities_08102021\\DRC_preprocess.gdb\\DRC_WHO_hf_merged\"\n",
    "input_data=r\"D:\\Grid3\\ISS\\processing\\{x}_WHO_health_facilities_08102021\\{x}_preprocess.gdb\\{x}_WHO_hf_merged\".format(x=country_code)\n",
    "# today date\n",
    "today_date=\"09012021\"    # format of the date \"MonthDAYYEAR\" exp:\"01012021\" >> this goes to into name of the  output layers\n",
    "\n",
    "# point type \n",
    "point_poi_type=\"hf\" # options >> health_facility, settlement, schools or other point of interests\n",
    "\n",
    "# source of dataset\n",
    "points_source=\"who\"\n",
    "\n",
    "\n",
    "# import type dictionary as TYPE_DICT\n",
    "path_to_type_dict =  r\"D:\\Grid3\\ISS\\inputs\\spelling_type_dict\\{x}_type_dict_augmented_1130.csv\".format(x=country_code)\n",
    "TYPE_DICT = pd.read_csv(path_to_type_dict )\n",
    "\n",
    "# import spelling dictionary as SPELLING_DICT\n",
    "path_to_spelling_dict = r\"D:\\Grid3\\ISS\\inputs\\spelling_type_dict\\{x}_spelling_dict_052021.csv\".format(x=country_code)\n",
    "SPELLING_DICT = pd.read_csv(path_to_spelling_dict )\n",
    "\n",
    "# master facility list\n",
    "MFL_path=r\"D:\\Grid3\\ISS\\processing\\MFL_by_country\\mfl_by_country.gdb\\{x}_mfl\".format(x=country_code)\n",
    "\n",
    "#path to settlement extent\n",
    "#bua, ssa and hamlets should be merged into a layer\n",
    "# sett_extent=r\"D:\\Grid3\\ISS\\inputs\\CMR\\CMR.gdb\\GRID3_Cameroon_Settlement_Extents_Version_1\"\n",
    "# dist_to_sett_threshold=\"250 Meters\" \n",
    "# dist_to_check_overlap=\"0.1 Meters\"\n",
    "\n",
    "# #Admin boundaries\n",
    "# Admin1_bndry=r\"D:\\Grid3\\ISS\\inputs\\CMR\\CMR.gdb\\cmr_admbnda_adm1_inc_20180104\"\n",
    "# ADMIN1_BNDRY=\"ADM1_FR\"\n",
    "\n",
    "# Admin2_bndry=r\"D:\\Grid3\\ISS\\inputs\\CMR\\CMR.gdb\\cmr_admbnda_adm3_inc_20180104\"\n",
    "# ADMIN2_BNDRY=\"ADM3_FR\"\n",
    "\n",
    "\n",
    "#path to settlement extent\n",
    "#bua, ssa and hamlets should be merged into a layer\n",
    "sett_extent=r\"D:\\Grid3\\\\DRC\\DRC_Health_Facilities\\Data\\Spatial_Data\\GRID3_DRC_settlement_extents_20200403_V02.gdb\\\\bua_ssa_hamlet\"\n",
    "dist_to_sett_threshold=\"250 Meters\" \n",
    "dist_to_check_overlap=\"0.1 Meters\"\n",
    "\n",
    "#Admin boundaries\n",
    "Admin1_bndry=r\"D:\\Grid3\\DRC\\DRC_Health_Facilities\\Data\\Spatial_Data\\ISS_hf_2020\\ISS_2017_2021.gdb\\DRC_admin1\"\n",
    "ADMIN1_BNDRY=\"ADM1_REF\"\n",
    "\n",
    "\n",
    "Admin2_bndry=r\"D:\\Grid3\\DRC\\DRC_Health_Facilities\\Data\\Spatial_Data\\ISS_hf_2020\\ISS_2017_2021.gdb\\DRC_admin2\"\n",
    "ADMIN2_BNDRY=\"Nom\"\n",
    "\n",
    "# country column in english\n",
    "COUNTRY = 'countries'\n",
    "\n",
    "# output columns\n",
    "ADMIN1=\"admin1\"\n",
    "ADMIN2=\"admin2\"\n",
    "ADMIN3=\"admin3\"\n",
    "FACILITY_NAME = 'org_name'\n",
    "CLEAN_NAME = 'clean_name' # clean name after some pre-cleaning\n",
    "CORRECT_NAME = 'corrected_name' # clean name after spelling correction\n",
    "CLEAN_NAME_FINAL = 'clean_name_final' # final clean name after removing type information\n",
    "EXTRACT_TYPE = 'type_extract' # type information extracted\n",
    "SUB_TYPE = 'type' # type mapped to the type dictonary\n",
    "SCORE = 'score' # match score between 'type_extract' and 'sub_type'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepaire Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directory\n",
    "if not  os.path.exists (os.path.join(root_dir,country_code+\"_WHO_health_facilities_\"+today_date)):\n",
    "    os.mkdir(os.path.join(root_dir,country_code+\"_WHO_health_facilities_\"+today_date))\n",
    "output_loc=os.path.join(root_dir,country_code+\"_WHO_health_facilities_\"+today_date)\n",
    "\n",
    "# create final output gdb\n",
    "if  not arcpy.Exists(os.path.join(output_loc,country_code+\"_preprocess.gdb\")):\n",
    "    arcpy.CreateFileGDB_management(output_loc,country_code+\"_preprocess.gdb\")\n",
    "output_gdb=os.path.join(output_loc,country_code+\"_preprocess.gdb\") \n",
    "\n",
    "# output\n",
    "OUT_FILE = country_code + \"_WHO_hf_preprocess\"\n",
    "SAVE_FILE = os.path.join(output_gdb, OUT_FILE)\n",
    "OUTPUT =os.path.join(output_gdb, OUT_FILE)\n",
    "OUTPUT_csv=  os.path.join(output_loc, OUT_FILE+\".csv\")                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> There are 122322 rows in the input layer\n",
      ">> There are 122292 rows remained after preprocessing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##calculate xy coordinates in meter. it necessary for clusterig\n",
    "## africa equal area projection is used. Change if it is necessary\n",
    "crf=\"102022\"\n",
    "arcpy.AddField_management(input_data,field_name='x_coord_m', field_type=\"DOUBLE\")\n",
    "arcpy.AddField_management(input_data,field_name='y_coord_m', field_type=\"DOUBLE\")\n",
    "arcpy.CalculateGeometryAttributes_management(input_data, [['x_coord_m', 'POINT_X'], \n",
    "                                                    ['y_coord_m', 'POINT_Y']],coordinate_system = crf)\n",
    "# read input dataset as spatial dataframe\n",
    "sdf=pd.DataFrame.spatial.from_featureclass(input_data)\n",
    "# exclude records that do not have lat/long\n",
    "sdf=sdf[sdf['x_coord_m'].notnull()]\n",
    "##===================================================================##\n",
    "print (f\">> There are {arcpy.GetCount_management(input_data).getOutput(0)} rows in the input layer\" )\n",
    "print (f\">> There are {sdf.shape[0]} rows remained after preprocessing\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting and Standardizig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Summary by type of facilities:\n",
      "Centre de Sante                              56547\n",
      "Centre Medical                                6333\n",
      "Hopital General de Reference                  4805\n",
      "Poste de Sante                                4094\n",
      "Centre Hospitalier                            3281\n",
      "Centre de Sante de Reference                  2825\n",
      "Clinique                                      1851\n",
      "Hopital                                        763\n",
      "Polyclinique                                   642\n",
      "Centre de Sante Municipal                      569\n",
      "Dispensaire                                    508\n",
      "Centre Medico-Chirurgical                      263\n",
      "Hopital Secondaire                             196\n",
      "Maternite                                      162\n",
      "Centre de Sante Clinique                        88\n",
      "Centre Pediatrique                              80\n",
      "Centre de Sante Maternite                       65\n",
      "Centre De Sante Et Maternite                    56\n",
      "Centre Medical Et Maternite                     35\n",
      "Centre de Sante and Maternite                   30\n",
      "Hopital Provincial de Reference                 28\n",
      "Cliniques Universitaires                        27\n",
      "Hopital Militaire                               27\n",
      "Cabinet Medical                                 23\n",
      "Centre Hospitalier Maternite                    19\n",
      "Centre de Rehabilitation                        18\n",
      "Centre Chirurgical                              15\n",
      "Hopital Militaire de Reference                  10\n",
      "Centre de Sante De Reference Et Maternite        9\n",
      "Centre Hopitalier                                6\n",
      "Centre de Sante Dispensaire                      1\n",
      "Name: type, dtype: int64\n",
      "\n",
      ">>> Poinst that are located in the right admin1 unit:\n",
      "YES    122093\n",
      "NO        260\n",
      "Name: admin1_bdry_match, dtype: int64\n",
      "\n",
      ">>> Poinst that are located in the right admin2 unit:\n",
      "YES    90711\n",
      "NO     31642\n",
      "Name: admin2_bdry_match, dtype: int64\n",
      "\n",
      ">>> Points summary based on settlement type based:\n",
      "bua                    94549\n",
      "ssa                    24842\n",
      "hamlet                  2417\n",
      "Out of a settlement      545\n",
      "Name: sett_type, dtype: int64\n",
      "\n",
      "\n",
      ">>> Points summary based on overlaps  0.1 Meters:\n",
      "NO     116021\n",
      "YES      6332\n",
      "Name: is_overlap, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cluster points\n",
    "df=create_cluster(sdf,1000, 2,\"cluster_id\")\n",
    "\n",
    "# pre-cleaning\n",
    "pre_cleaned_res = preclean(df, facility_name = FACILITY_NAME, clean_name = CLEAN_NAME,\n",
    "                          country_col = COUNTRY)\n",
    "# make spelling correction\n",
    "corrected_results = correct_spelling(pre_cleaned_res, spelling_dict=SPELLING_DICT, country_col=COUNTRY, \n",
    "                                     clean_name = CLEAN_NAME, output_col=CORRECT_NAME)\n",
    "# remove type information\n",
    "res = remove_type_info(corrected_results, type_dict=TYPE_DICT, clean_name=CORRECT_NAME, \n",
    "                       clean_name_final=CLEAN_NAME_FINAL, country=COUNTRY)\n",
    "# obtain facility type extracted\n",
    "extract_type(df=res, clean_name=CORRECT_NAME, \n",
    "             clean_name_final=CLEAN_NAME_FINAL, extract_type=EXTRACT_TYPE)\n",
    "res = map_type(df=res, country = COUNTRY, extract_type=EXTRACT_TYPE, \n",
    "               sub_type=SUB_TYPE, score=SCORE, type_dict=TYPE_DICT)\n",
    "# complite missing types\n",
    "complite_type=complite_types(res)\n",
    "complite_type.drop('cluster_id_1000m', inplace=True, axis=1)\n",
    "# export the result\n",
    "complite_type.spatial.to_featureclass(OUTPUT)\n",
    "#check by if points in the right admin1\n",
    "check_by_admin(OUTPUT,ADMIN1, Admin1_bndry,ADMIN1_BNDRY)\n",
    "\n",
    "# check by if points in the right admin2\n",
    "check_by_admin(OUTPUT,ADMIN2, Admin2_bndry,ADMIN2_BNDRY)\n",
    "# check by settlement extent\n",
    "check_by_settlement(OUTPUT, sett_extent, dist_to_sett_threshold)\n",
    "\n",
    "# check by overlaps\n",
    "check_overlaps(OUTPUT,dist_to_check_overlap)\n",
    "\n",
    "##===================Result=========================##\n",
    "result=pd.DataFrame.spatial.from_featureclass(OUTPUT)\n",
    "print(\">>> Summary by type of facilities:\")\n",
    "print(result[SUB_TYPE].value_counts())\n",
    "print()\n",
    "print(\">>> Poinst that are located in the right admin1 unit:\")\n",
    "print(result[ADMIN1+\"_bdry_match\"].value_counts())\n",
    "print()\n",
    "print(\">>> Poinst that are located in the right admin2 unit:\")\n",
    "print(result[ADMIN2+\"_bdry_match\"].value_counts())\n",
    "print()\n",
    "print(\">>> Points summary based on settlement type based:\")\n",
    "print(result[\"sett_type\"].value_counts())\n",
    "print()\n",
    "print()\n",
    "print(f\">>> Points summary based on overlaps  {dist_to_check_overlap}:\")\n",
    "print(result[\"is_overlap\"].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match  with master facility list (pyramid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepaire MFL for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input data\n",
    "MFL=pd.DataFrame.spatial.from_featureclass(\"DRC_mfl\")\n",
    "# drop duplicated recor in order to avoid duplicate match \n",
    "MFL.drop_duplicates(subset=[\"mfl_uuid\"], inplace=True)\n",
    "\n",
    "## input variables from MFL\n",
    "MFL_ADMIN1='adm1_name'\n",
    "MFL_ADMIN2='adm2_name'\n",
    "MFL_ADMIN3='adm3_name'\n",
    "MFL_FACE_NAME='facility_short'\n",
    "MFL_FACE_TYPE='type_clean'\n",
    "MFL_ID='mfl_uuid'\n",
    "\n",
    "# standardized admin names\n",
    "MFL_ADMIN1_2=MFL_ADMIN1+\"_2\"\n",
    "MFL=preclean_admin(MFL,MFL_ADMIN1, MFL_ADMIN1_2)\n",
    "MFL_ADMIN2_2=MFL_ADMIN2+\"_2\"\n",
    "MFL=preclean_admin(MFL,MFL_ADMIN2, MFL_ADMIN2_2)\n",
    "\n",
    "## get list of ADMIN1, ADMIN2and ADMIN3 from MFL\n",
    "MFL_ADMIN1_list=MFL[MFL_ADMIN1_2].unique().tolist()\n",
    "MFL_ADMIN2_list=MFL[MFL_ADMIN2_2].unique().tolist()\n",
    "#MFL_ADMIN3_list=MFL[MFL_ADMIN3].unique().tolist()\n",
    "\n",
    "# remove accents in MFL\n",
    "def remove_accents(a):\n",
    "    if isinstance(a, str):\n",
    "        return unidecode.unidecode(a)\n",
    "\n",
    "MFL[MFL_ADMIN2]= MFL[MFL_ADMIN2].apply(remove_accents)\n",
    "#MFL[MFL_ADMIN3]= MFL[MFL_ADMIN3].apply(remove_accents)\n",
    "MFL[MFL_FACE_NAME]= MFL[MFL_FACE_NAME].apply(remove_accents)\n",
    "MFL[MFL_FACE_TYPE]= MFL[MFL_FACE_TYPE].apply(remove_accents)\n",
    "\n",
    "\n",
    "\n",
    "## Flag health facilities with the same name but different types \n",
    "## it will help to prevent duplicate match because type will not be used for the match process\n",
    "groupby_admin1_2=MFL.groupby( [MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_FACE_NAME]).size().reset_index(name=\"count1\")\n",
    "MFL=MFL.merge(groupby_admin1_2, left_on=[MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_FACE_NAME], \n",
    "                      right_on=[MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_FACE_NAME])\n",
    "\n",
    "## Flag health facilities that have the same name and type by health zone\n",
    "## if the facility name duplicated, the duplicated records will be mathced based on \n",
    "## health zone, health area and facility name\n",
    "groupby_admin1=MFL.groupby( [MFL_ADMIN1_2,MFL_FACE_NAME]).size().reset_index(name=\"count2\")\n",
    "MFL=MFL.merge(groupby_admin1, left_on=[MFL_ADMIN1_2,MFL_FACE_NAME], \n",
    "                      right_on=[MFL_ADMIN1_2,MFL_FACE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_admin1_2[groupby_admin1_2[\"count1\"]>=2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1281, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_admin1[groupby_admin1[\"count2\"]>=2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching admin1 (province/states) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############=== admin1 matcth result ===#####################\n",
      " 26 admin 2 matched between input data and MFL\n",
      " 0 admin 2 did not matched between input data and MFL\n"
     ]
    }
   ],
   "source": [
    "result=pd.DataFrame.spatial.from_featureclass(OUTPUT)\n",
    "# check by province_name\n",
    "# format province names from imput data\n",
    "#  new caloums to save match result and correct names if it matched\n",
    "ADMIN1_2=\"clean_\"+ADMIN1\n",
    "ADMIN1_matched=ADMIN1_2+\"_isMatch\"\n",
    "match_admin1=preclean_admin(result,ADMIN1, ADMIN1_2)\n",
    "\n",
    "admin1_group=match_admin1.groupby(ADMIN1_2)\n",
    "#match admin1 between MFL and the input data\n",
    "for i, group in admin1_group:\n",
    "    match_name, score = process.extractOne(i, MFL_ADMIN1_list)\n",
    "    # match score above 80 will be true match and admin1 from the input data \n",
    "    # will be changed with admin1 from MFL\n",
    "    if score >=80:\n",
    "        match_admin1.loc[match_admin1[ADMIN1_2]==i, ADMIN1_matched] =\"YES\"\n",
    "        match_admin1.loc[match_admin1[ADMIN1_2]==i, ADMIN1_2] = match_name\n",
    "        \n",
    "    # match score less than 80 will be false match and \n",
    "    # admin1 name from the input data will be kept.\n",
    "    # the rows that thier admin1 did not match will not go \n",
    "    # next step matching process. Manually check admin1  if it is needed\n",
    "    else:\n",
    "        match_admin1.loc[match_admin1[ADMIN1_2]==i, ADMIN1_matched] =\"NO\"\n",
    "\n",
    "##==========================================================================##\n",
    "# admin1 match result\n",
    "match_count=len(match_admin1[match_admin1[ADMIN1_matched] ==\"YES\"][ADMIN1_2].unique())\n",
    "notMatch_count=len(match_admin1[match_admin1[ADMIN1_matched] ==\"NO\"][ADMIN1_2].unique())\n",
    "print(\"##############=== admin1 matcth result ===#####################\")\n",
    "print ( f\" {match_count} admin 2 matched between input data and MFL\")\n",
    "print ( f\" {notMatch_count} admin 2 did not matched between input data and MFL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching admin2  ( district/health zone) names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##=== admin2 matcth result ===###\n",
      " 477 admin 2 matched between input data and MFL\n",
      " 25 admin 2 did not matched between input data and MFL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# format admin1 names from input data\n",
    "#  new caloums to save match result and correct names if it matched\n",
    "ADMIN2_2=\"clean_\"+ADMIN2\n",
    "ADMIN2_matched=ADMIN2_2+\"_isMatch\"\n",
    "match_admin2=preclean_admin(match_admin1,ADMIN2,ADMIN2_2)\n",
    "\n",
    "# create unique id column with combination of admin1, admin2\n",
    "# the unique id will be used for cheking if admin2 match to MFL admin2\n",
    "match_admin1[ADMIN2_matched]=\"\"\n",
    "match_admin2[\"uniqueid\"]=match_admin2[ADMIN1_2]+\"_\"+match_admin2[ADMIN2_2]\n",
    "# match admin2 name by each  admin1\n",
    "# the match process will be limited to admin1\n",
    "match_admin2[ADMIN2_matched]=\"\"\n",
    "for admin1_ in MFL_ADMIN1_list:\n",
    "        # get match candidates from MFL\n",
    "    match_candiates=MFL[MFL[MFL_ADMIN1_2]==admin1_][MFL_ADMIN2_2].unique().tolist() \n",
    "    # get admin2 names to be match to MFL admin2\n",
    "    match_df=match_admin2[match_admin2[ADMIN1_2]==admin1_]\n",
    "    if match_df.shape[0]>=1:\n",
    "    # group by admin2 in order to make matching process shorther\n",
    "    # since admin2 is repeated many times. So each admin2 will be match only once and\n",
    "    # then all rows with the same admin2 will be changed \n",
    "        admin2_group=match_df.groupby([ADMIN1_2, ADMIN2_2])\n",
    "        for i, group in admin2_group:\n",
    "            get_admin2_name=i[1]\n",
    "            # create unique id from each groups to update match result \n",
    "            # in the input data\n",
    "            get_index=i[0]+\"_\"+i[1]\n",
    "            # matching\n",
    "            match_name, score = process.extractOne(get_admin2_name, match_candiates, scorer=fuzz.token_sort_ratio)\n",
    "            # match score above 80 will be true match and admin2 from the input data \n",
    "            # will be changed with admin2 from MFL\n",
    "            if score >=80:\n",
    "                match_admin2.loc[match_admin2[\"uniqueid\"]==get_index, ADMIN2_matched] =\"YES\"\n",
    "                match_admin2.loc[match_admin2[\"uniqueid\"]==get_index, ADMIN2_2] = match_name\n",
    "                \n",
    "                # match score less than 80 will be false match and \n",
    "                # admin2 name from the input data will be kept.\n",
    "                # the rows that thier admin2 did not match will not go \n",
    "                # next step matching process. Manually check admin1  if it is needed\n",
    "            else:\n",
    "                match_admin2.loc[match_admin2[\"uniqueid\"]==get_index, ADMIN2_matched] =\"NO\"\n",
    "# drop unique id column                        \n",
    "match_admin2.drop(\"uniqueid\", axis=1, inplace=True)\n",
    "##==========================================================================##\n",
    "# admin 2 match result\n",
    "match_count=len(match_admin2[match_admin2[ADMIN2_matched] ==\"YES\"][ADMIN2_2].unique())\n",
    "notMatch_count=len(match_admin2[match_admin2[ADMIN2_matched] ==\"NO\"][ADMIN2_2].unique())\n",
    "print(\"##=== admin2 matcth result ===###\")\n",
    "print ( f\" {match_count} admin 2 matched between input data and MFL\")\n",
    "print ( f\" {notMatch_count} admin 2 did not matched between input data and MFL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching  admin3 (wards/health area) names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# format admin3 names from input data\n",
    "# new caloums to save match result and correct names if it matched\n",
    "ADMIN3_2=\"clean_\"+ADMIN3\n",
    "ADMIN3_matched=ADMIN3_2+\"_isMatch\"\n",
    "match_admin3=preclean_admin(match_admin2,ADMIN3, ADMIN3_2)\n",
    "\n",
    "\n",
    "# # create unique id column with combination of admin1, admin2, and admin3\n",
    "# # the unique id will be used for cheking if admin3 match to MFL admin3\n",
    "# match_admin3[ADMIN3_matched]=\"\"\n",
    "# match_admin3[\"uniqueid\"]=match_admin3[ADMIN1_2]+\"_\"+match_admin3[ADMIN2_2]+\"_\"+match_admin3[ADMIN3_2]\n",
    "\n",
    "# # matching process limited to admin1 and admin2.\n",
    "# # iterate by admin1\n",
    "# for admin1_ in MFL_ADMIN1_list:\n",
    "#     selected_admin2=MFL[MFL[MFL_ADMIN1]==admin1_][MFL_ADMIN2].unique().tolist()\n",
    "  \n",
    "#     # itarate by each admin2\n",
    "#     for admin2_ in selected_admin2:\n",
    "#         # get match candidates from MFL\n",
    "#         match_candiates=MFL[(MFL[MFL_ADMIN1]==admin1_) &(MFL[MFL_ADMIN2]==admin2_)]\\\n",
    "#         [MFL_ADMIN3].unique().tolist() \n",
    "#         # get admin3 names to be match to MFL admin3\n",
    "#         match_df=match_admin3[(match_admin3[ADMIN1_2]==admin1_) & (match_admin3[ADMIN2_2]==admin2_)]\n",
    "#         if match_df.shape[0]>=1:\n",
    "#             # group by admin3 in order to make matching process shorther\n",
    "#             # since admin3 is repeated many times. So each admin3 will be match only once and\n",
    "#             # then all rows with the same admin3 will be changed \n",
    "#             admin3_group=match_df.groupby([ADMIN1_2, ADMIN2_2,ADMIN3_2])\n",
    "#             for i, group in admin3_group:\n",
    "\n",
    "#                 get_admin2_name=i[2]\n",
    "#                 # create unique id from each groups to update match result \n",
    "#                 # in the input data\n",
    "#                 get_index=i[0]+\"_\"+i[1]+\"_\"+i[2]\n",
    "#                 # matching\n",
    "#                 match_name, score = process.extractOne(get_admin2_name, match_candiates)\n",
    "#                 # short names gives low matching score even if a letter differnce\n",
    "#                 # different score threshold is used based on matching name lenght\n",
    "#                 # true match and admin2 from the input data \n",
    "#                 # will be changed with admin3 from MFL\n",
    "#                 if (len(get_admin2_name) <=4) & (score >=70):\n",
    "#                     match_admin3.loc[match_admin3[\"uniqueid\"]==get_index,ADMIN3_matched]=\"YES\"\n",
    "#                     match_admin3.loc[match_admin3[\"uniqueid\"]==get_index,ADMIN3_2]=match_name\n",
    "                    \n",
    "#                 if (len(get_admin2_name)==5) & (score >=80):\n",
    "#                     match_admin3.loc[match_admin3[\"uniqueid\"]==get_index,ADMIN3_matched]=\"YES\"\n",
    "#                     match_admin3.loc[match_admin3[\"uniqueid\"]==get_index,ADMIN3_2]=match_name\n",
    "                    \n",
    "#                 if score >=85:\n",
    "#                     match_admin3.loc[match_admin3[\"uniqueid\"]==get_index,ADMIN3_matched]=\"YES\"\n",
    "#                     match_admin3.loc[match_admin3[\"uniqueid\"]==get_index,ADMIN3_2]=match_name\n",
    "                    \n",
    "#                     # match score less than 80 will be false match and \n",
    "#                     # admin2 name from the input data will be kept.\n",
    "#                     # the rows that thier admin2 did not match will not go \n",
    "#                     # next step matching process. Manually check admin1  if it is needed\n",
    "#                 else:\n",
    "#                     match_admin3.loc[match_admin3[\"uniqueid\"]==get_index,ADMIN3_matched]=\"NO\"\n",
    "                 \n",
    "                    \n",
    "# # drop unique id column                        \n",
    "# match_admin3.drop(\"uniqueid\", axis=1, inplace=True)\n",
    "# ##==========================================================================##\n",
    "# # admin3 match result\n",
    "# match_count=len(match_admin3[match_admin3[ADMIN3_matched] ==\"YES\"][ADMIN3_2].unique())\n",
    "# notMatch_count=len(match_admin3[match_admin3[ADMIN3_matched] ==\"NO\"][ADMIN3_2].unique())\n",
    "# print(\"##=== admin3  matcth result ===###\")\n",
    "# print ( f\" {match_count} admin 3 matched between input data and MFL\")\n",
    "# print ( f\" {notMatch_count} admin 3 did not matched between input data and MFL\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching facility names based on admin1, admin2 and facility name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##=== facility name matcth result (based on admin1, admin2) ===##\n",
      " >>> 3955 facility name matched between input data and MFL\n",
      " >>> 15094 facility name did not matched between input data and MFL\n"
     ]
    }
   ],
   "source": [
    "def return_match(df,index_,match_name, match_uuid, match_result, match_score, match_type):    \n",
    "    df.loc[df[\"uniqueid\"]==index_,\"face_name2\"]=match_name\n",
    "    df.loc[df[\"uniqueid\"]==index_,\"mfl_match_name\"]=match_name\n",
    "    df.loc[df[\"uniqueid\"]==index_,\"mfl_match_uuid\"]=mfl_uuid1\n",
    "    df.loc[df[\"uniqueid\"]==index_,\"mfl_match_result\"]=match_result\n",
    "    df.loc[df[\"uniqueid\"]==index_,\"mfl_match_score\"]=match_score\n",
    "    df.loc[df[\"uniqueid\"]==index_,\"mfl_match_type\"]=match_type\n",
    "\n",
    "## facility name will be match in two different scenario\n",
    "# 1. If health area match, the  match will be based on health zone, health area and facility name\n",
    "# 2. If health areas did not match, he match will be based on health zone and facility name\n",
    "\n",
    "\n",
    "# add new fields to save match\n",
    "match_admin3[\"face_name2\"]=match_admin3[\"clean_name_final\"]\n",
    "match_admin3[\"mfl_match_name\"]=\"\"\n",
    "match_admin3[\"mfl_match_uuid\"]=\"\"\n",
    "match_admin3[\"mfl_match_result\"]=\"\"\n",
    "match_admin3[\"mfl_match_score\"]=0\n",
    "match_admin3[\"mfl_match_type\"]=\"\"\n",
    "#match for cases if admin3 match between Mfl and input data\n",
    "fname_match1=match_admin3[match_admin3[ADMIN2_matched] ==\"YES\"]\n",
    "\n",
    "# 1. Match will be based on health zone, health area and facility name\n",
    "# get clean facility name\n",
    "# create unique id column with combination of admin1, admin2, admin3 and facility name\n",
    "# the unique id will be used for cheking if facility name match to MFL facility name\n",
    "fname_match1[\"uniqueid\"]=fname_match1[ADMIN1_2]+\"_\"+fname_match1[ADMIN2_2]+\"_\"+fname_match1[\"face_name2\"]\n",
    "\n",
    "for admin1_ in MFL_ADMIN1_list:\n",
    "    selected_admin2=MFL[MFL[MFL_ADMIN1_2]==admin1_][MFL_ADMIN2_2].unique().tolist()  \n",
    "    for admin2_ in selected_admin2:   \n",
    "        match_candiates=MFL[(MFL[MFL_ADMIN1_2]==admin1_) &\\\n",
    "                         (MFL[MFL_ADMIN2_2]==admin2_)][MFL_FACE_NAME].unique().tolist()\n",
    "        match_df=fname_match1[(fname_match1[ADMIN1_2]==admin1_) & (fname_match1[ADMIN2_2]==admin2_)]\n",
    "        if match_df.shape[0]>=1:\n",
    "            # group by face name in order to make matching process shorther\n",
    "            # since face name is repeated many times. So each face name will be match only once and\n",
    "                # then all rows with the same face name will be changed \n",
    "            fname_match1_group=match_df.groupby([ADMIN1_2, ADMIN2_2, \"face_name2\"])\n",
    "            for i, group in fname_match1_group:\n",
    "                get_fname_name=i[2]\n",
    "                get_index=i[0]+\"_\"+i[1]+\"_\"+i[2]\n",
    "                if len(match_candiates)>=1:\n",
    "                    match_name, score = process.extractOne(get_fname_name, match_candiates, scorer=fuzz.token_sort_ratio)\n",
    "                    match_name2, score2 = process.extractOne(get_fname_name, match_candiates, scorer=fuzz.partial_ratio)\n",
    "                    \n",
    "                    mfl_uuid1=\"/\".join(MFL[(MFL[MFL_ADMIN1_2]==admin1_)&(MFL[MFL_ADMIN2_2]==admin2_)&(MFL[MFL_FACE_NAME]==match_name)][\"mfl_uuid\"].tolist())\n",
    "                    mfl_uuid2=\"/\".join(MFL[(MFL[MFL_ADMIN1_2]==admin1_)&(MFL[MFL_ADMIN2_2]==admin2_)&(MFL[MFL_FACE_NAME]==match_name2)][\"mfl_uuid\"].tolist())\n",
    "            \n",
    "               \n",
    "                    if len(get_fname_name) <=4 and score >=75:\n",
    "                        return_match(fname_match1,get_index, match_name, mfl_uuid1,\"YES\", score, \"Simple\")  \n",
    "                        if score>=90:\n",
    "                            match_candiates.remove(match_name)\n",
    "                        continue\n",
    "                    elif len(get_fname_name) ==5 and score >=80: \n",
    "                        return_match(fname_match1,get_index, match_name,mfl_uuid1,\"YES\", score, \"Simple\") \n",
    "                        if score>=90:\n",
    "                            match_candiates.remove(match_name)\n",
    "                        continue\n",
    "                    elif len(get_fname_name) >5 and score >=83:\n",
    "                        return_match(fname_match1,get_index, match_name, mfl_uuid1,\"YES\", score, \"Simple\") \n",
    "                        if score>=90:\n",
    "                            match_candiates.remove(match_name)\n",
    "                        continue\n",
    "                    elif len(get_fname_name) >=5 and score <83 and score2==100:\n",
    "                        return_match(fname_match1,get_index, match_name2, mfl_uuid2,\"YES\", score2, \"Partial\") \n",
    "                        if score==100:\n",
    "                            match_candiates.remove(match_name2)\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        fname_match1.loc[fname_match1[\"uniqueid\"]==get_index,\"mfl_match_result\"]=\"NO\"\n",
    "                        fname_match1.loc[fname_match1[\"uniqueid\"]==get_index,\"mfl_match_score\"]=score\n",
    "                        fname_match1.loc[fname_match1[\"uniqueid\"]==get_index,\"mfl_match_name\"]=match_name\n",
    "                        fname_match1.loc[fname_match1[\"uniqueid\"]==get_index,\"mfl_match_uuid\"]=mfl_uuid1\n",
    "                        \n",
    "fname_match1.drop(\"uniqueid\", axis=1, inplace=True)\n",
    "fname_match1_yes=fname_match1[fname_match1[\"mfl_match_result\"] ==\"YES\"]\n",
    "##==========================================================================##\n",
    "# ## match by health zone health area and facility name\n",
    "# ## exclude if there are facilities with the same but different type\n",
    "# fname_match1_MFL=fname_match1_.merge(MFL[[MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_ID,MFL_FACE_NAME, MFL_FACE_TYPE,\n",
    "#                                                \"hf_code\", 'duplicate_uuid', 'count1']],\n",
    "#                                      left_on=[ADMIN2_2, \"face_name2\"],\n",
    "#                                      right_on=[MFL_ADMIN2_2,MFL_FACE_NAME],how=\"left\")\n",
    "\n",
    "\n",
    "# # chnage type of the facility to MFL if facility name is unique in admin1\n",
    "# for index, row in fname_match1_MFL.iterrows():\n",
    "#     # if type of facility unique for each facility name ( 'pyrmd_count1' !=1)\n",
    "#     # replace type to be identical to MFL\n",
    "#     if fname_match1_MFL.at[index, 'count1']==1: \n",
    "#         fname_match1_MFL.at[index,SUB_TYPE]=fname_match1_MFL.at[index,MFL_FACE_TYPE]\n",
    "# ## rematch facilities if facility name is duplicated in the same admin1 \n",
    "# ## count column in MFL indicates duplication of facility name: count2==2\n",
    "# ## seperate facilities that may duplicate match\n",
    "# ## non duplicated\n",
    "# non_duplicate=fname_match1_MFL[(fname_match1_MFL['count1']<2)| (fname_match1_MFL['count1'].isnull())]  \n",
    "# ## may duplicated\n",
    "# get_duplicate_match=fname_match1_MFL[fname_match1_MFL['count1']>=2]\n",
    "# ## drop MFL columns from previous match \n",
    "# get_duplicate_match.drop([MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_ID,MFL_FACE_NAME, MFL_FACE_TYPE,\n",
    "#                             \"hf_code\", 'duplicate_uuid', 'count1'], axis=1, inplace=True)\n",
    "# ## drop duplicate cases from in the first match by using uuid column \n",
    "# get_duplicate_match.drop_duplicates([\"uuid\"], inplace=True)\n",
    "# ## rematch with MFL by adding facility type in the match list\n",
    "# get_duplicate_match=get_duplicate_match.merge(MFL[[MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_ID,MFL_FACE_NAME, MFL_FACE_TYPE,\n",
    "#                                                \"hf_code\", 'duplicate_uuid', 'count1']],\n",
    "#                                      left_on=[ADMIN2_2, \"face_name2\",SUB_TYPE],\n",
    "#                                      right_on=[MFL_ADMIN2_2,MFL_FACE_NAME,MFL_FACE_TYPE],how=\"left\")\n",
    "# ## remerge \n",
    "# fname_match1_MFL2=pd.concat([non_duplicate,get_duplicate_match]) \n",
    "\n",
    "##==========================================================================##\n",
    "\n",
    "match_count1=len(fname_match1[fname_match1[\"mfl_match_result\"] ==\"YES\"][\"face_name2\"].unique())\n",
    "match_count2=len(fname_match1[fname_match1[\"mfl_match_result\"] ==\"NO\"][\"face_name2\"].unique())\n",
    "print(\"##=== facility name matcth result (based on admin1, admin2) ===##\")\n",
    "print ( f\" >>> {match_count1} facility name matched between input data and MFL\")\n",
    "print  ( f\" >>> {match_count2} facility name did not matched between input data and MFL\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching facility names based on admin1, admin2 and facility name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##=== facility name matcth result (based on admin2)===##\n",
      ">>> 2991 admin 3 matched between input data and MFL\n",
      ">>> 11714 admin 3 did not matched between input data and MFL\n"
     ]
    }
   ],
   "source": [
    "# 2. If health areas did not match, he match will be based on health zone and facility name\n",
    "\n",
    "fname_match1_no=fname_match1[(fname_match1[\"mfl_match_result\"] ==\"NO\")|(fname_match1[\"mfl_match_result\"] ==\"\")]\n",
    "fname_notMatch2_no=match_admin3[match_admin3[ADMIN2_matched] ==\"NO\"]\n",
    "\n",
    "fname_notMatch2=pd.concat([fname_match1_no,fname_notMatch2_no])                              \n",
    "                              \n",
    "# 1. Match will be based on health zone, health area and facility name\n",
    "fname_notMatch2[\"uniqueid\"]=fname_notMatch2[ADMIN1_2]+\"_\"+fname_notMatch2[\"face_name2\"]\n",
    "\n",
    "for admin1_ in MFL_ADMIN1_list:  \n",
    "    match_candiates=MFL[MFL[MFL_ADMIN1_2]==admin1_][MFL_FACE_NAME].unique().tolist()   \n",
    "    match_df=fname_notMatch2[fname_notMatch2[ADMIN1_2]==admin1_]\n",
    "    if match_df.shape[0]>=1:\n",
    "        # group by face name in order to make matching process shorther\n",
    "        # since face name is repeated many times. So each face name will be match only once and\n",
    "        # then all rows with the same face name will be changed \n",
    "        fname_notMatch2_group=match_df.groupby([ADMIN1_2, \"face_name2\"])\n",
    "        for i, group in fname_notMatch2_group:\n",
    "            get_fname_name=i[1]\n",
    "            get_index=i[0]+\"_\"+i[1]\n",
    "            if len(match_candiates)>=1:\n",
    "                match_name, score = process.extractOne(get_fname_name, match_candiates, scorer=fuzz.token_sort_ratio)\n",
    "                match_name2, score2 = process.extractOne(get_fname_name, match_candiates, scorer=fuzz.partial_ratio)\n",
    "                \n",
    "                mfl_uuid1=\"/\".join(MFL[(MFL[MFL_ADMIN1_2]==admin1_)&(MFL[MFL_FACE_NAME]==match_name)][\"mfl_uuid\"].tolist())\n",
    "                mfl_uuid2=\"/\".join(MFL[(MFL[MFL_ADMIN1_2]==admin1_)&(MFL[MFL_FACE_NAME]==match_name2)][\"mfl_uuid\"].tolist())\n",
    "            \n",
    "              \n",
    "                if len(get_fname_name) <=4 and score >=75:\n",
    "                    return_match(fname_notMatch2,get_index, match_name, mfl_uuid1,\"YES\", score, \"Simple\")  \n",
    "                    if score>=90:\n",
    "                        match_candiates.remove(match_name)\n",
    "                    continue\n",
    "                elif len(get_fname_name) ==5 and score >=80: \n",
    "                    return_match(fname_notMatch2,get_index, match_name,mfl_uuid1,\"YES\", score, \"Simple\") \n",
    "                    if score>=90:\n",
    "                        match_candiates.remove(match_name)\n",
    "                    continue\n",
    "                elif len(get_fname_name) >5 and score >=83:\n",
    "                    return_match(fname_notMatch2,get_index, match_name, mfl_uuid1,\"YES\", score, \"Simple\") \n",
    "                    if score>=90:\n",
    "                        match_candiates.remove(match_name)\n",
    "                    continue\n",
    "                elif len(get_fname_name) >=5 and score <83 and score2==100:\n",
    "                    return_match(fname_notMatch2,get_index, match_name2, mfl_uuid2,\"YES\", score2, \"Partial\") \n",
    "                    if score==100:\n",
    "                        match_candiates.remove(match_name2)\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    fname_notMatch2.loc[fname_notMatch2[\"uniqueid\"]==get_index,\"mfl_match_result\"]=\"NO\"\n",
    "                    fname_notMatch2.loc[fname_notMatch2[\"uniqueid\"]==get_index,\"mfl_match_score\"]=score\n",
    "                    fname_notMatch2.loc[fname_notMatch2[\"uniqueid\"]==get_index,\"mfl_match_name\"]=match_name\n",
    "                    fname_notMatch2.loc[fname_notMatch2[\"uniqueid\"]==get_index,\"mfl_match_uuid\"]=mfl_uuid1\n",
    "                    \n",
    "fname_notMatch2.drop(\"uniqueid\", axis=1, inplace=True)\n",
    "\n",
    "# ##==========================================================================##\n",
    "# ## match by health zone health area and facility name\n",
    "# ## exclude if there are facilities with the same but different type\n",
    "# fname_notMatch2_MFL=fname_notMatch2.merge(MFL[[MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_ID,MFL_FACE_NAME, MFL_FACE_TYPE,\n",
    "#                                                \"hf_code\", 'duplicate_uuid', 'count2']],\n",
    "#                                      left_on=[ADMIN1_2, \"face_name2\"],\n",
    "#                                      right_on=[MFL_ADMIN1_2,MFL_FACE_NAME],how=\"left\")\n",
    "\n",
    "\n",
    "# # chnage type of the facility to MFL if facility name is unique in admin1\n",
    "# for index, row in fname_notMatch2_MFL.iterrows():\n",
    "#     # if type of facility unique for each facility name ( 'pyrmd_count1' !=1)\n",
    "#     # replace type to be identical to MFL\n",
    "#     if fname_notMatch2_MFL.at[index, 'count2']==1: \n",
    "#         fname_notMatch2_MFL.at[index,SUB_TYPE]=fname_notMatch2_MFL.at[index,MFL_FACE_TYPE]\n",
    "# ## rematch facilities if facility name is duplicated in the same admin1 \n",
    "# ## count column in MFL indicates duplication of facility name: count2==2\n",
    "# ## seperate facilities that may duplicate match\n",
    "# ## non duplicated\n",
    "# non_duplicate=fname_notMatch2_MFL[(fname_notMatch2_MFL['count2']<2)| (fname_notMatch2_MFL['count2'].isnull())]  \n",
    "# ## may duplicated\n",
    "# get_duplicate_match=fname_notMatch2_MFL[fname_notMatch2_MFL['count2']>=2]\n",
    "# ## drop MFL columns from previous match \n",
    "# get_duplicate_match.drop([MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_ID,MFL_FACE_NAME, MFL_FACE_TYPE,\n",
    "#                             \"hf_code\", 'duplicate_uuid', 'count2'], axis=1, inplace=True)\n",
    "# ## drop duplicate cases from in the first match by using uuid column \n",
    "# get_duplicate_match.drop_duplicates([\"uuid\"], inplace=True)\n",
    "# ## rematch with MFL by adding facility type in the match list\n",
    "# get_duplicate_match=get_duplicate_match.merge(MFL[[MFL_ADMIN1_2,MFL_ADMIN2_2,MFL_ID,MFL_FACE_NAME, MFL_FACE_TYPE,\n",
    "#                                                \"hf_code\", 'duplicate_uuid', 'count2']],\n",
    "#                                      left_on=[ADMIN2_2, \"face_name2\",SUB_TYPE],\n",
    "#                                      right_on=[MFL_ADMIN1_2,MFL_FACE_NAME,MFL_FACE_TYPE],how=\"left\")\n",
    "# ## remerge \n",
    "# fname_notMatch2_MFL2=pd.concat([non_duplicate,get_duplicate_match])            \n",
    "\n",
    "##==========================================================================##\n",
    "match_count=len(fname_notMatch2[fname_notMatch2[\"mfl_match_result\"] ==\"YES\"][\"face_name2\"].unique())\n",
    "notMatch_count=len(fname_notMatch2[fname_notMatch2[\"mfl_match_result\"] ==\"NO\"][\"face_name2\"].unique())\n",
    "print(\"##=== facility name matcth result (based on admin2)===##\")\n",
    "print ( f\">>> {match_count} admin 3 matched between input data and MFL\")\n",
    "print ( f\">>> {notMatch_count} admin 3 did not matched between input data and MFL\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "merge_all=pd.concat([fname_match1_yes,fname_notMatch2])\n",
    "\n",
    "\n",
    "# drop duplicated match by uuid (uuid is uique for each row)\n",
    "merge_all.drop_duplicates(subset=['uuid'], inplace=True)\n",
    "\n",
    "# fill empty rows with \"NA\" for admin1, admin2, admin3, facility name, and type\n",
    "merge_all['face_name2'].fillna(\"NA\",inplace=True)\n",
    "merge_all['clean_name_final'].fillna(\"NA\",inplace=True)\n",
    "merge_all[SUB_TYPE].fillna(\"NA\",inplace=True)\n",
    "merge_all[ADMIN1_2].fillna(\"NA\",inplace=True)\n",
    "merge_all[ADMIN2_2].fillna(\"NA\",inplace=True)\n",
    "merge_all[ADMIN3_2].fillna(\"NA\",inplace=True)\n",
    "\n",
    "merge_all.spatial.to_featureclass(OUTPUT)     \n",
    "merge_all.to_csv(OUTPUT_csv )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get count of word frequency\n",
    "word_count=merge_all[CLEAN_NAME_FINAL].str.split(expand=True).stack().value_counts()\n",
    "word_count_df=pd.DataFrame(word_count).reset_index().rename({0:\"frequancy\", \"index\":\"abrv\"}, axis=1)\n",
    "word_count_df=word_count_df[(word_count_df[\"abrv\"].str.len()<=4)&(word_count_df[\"frequancy\"]>=10)]\n",
    "word_count_df.to_csv(output_loc+\"\\\\abrv_frequancy.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get count of facility type\n",
    "type_count=merge_all['type_extract'].value_counts()\n",
    "type_count_df=pd.DataFrame(type_count).reset_index().rename({0:\"frequancy\", \"index\":\"type\"}, axis=1)\n",
    "type_count_df.to_csv(output_loc+\"\\\\type_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=result[CLEAN_NAME_FINAL].str.split(expand=True).stack().value_counts()\n",
    "word_count_df=pd.DataFrame(word_count).reset_index().rename({0:\"frequancy\", \"index\":\"abrv\"}, axis=1)\n",
    "word_count_df=word_count_df[(word_count_df[\"abrv\"].str.len()<=4)&(word_count_df[\"frequancy\"]>=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame.spatial.from_featureclass(\"DRC_mfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "alist=result[\"facility_short\"].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
